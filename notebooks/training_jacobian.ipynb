{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/browse/PackageIn\\\n",
      "fo.g:73\n",
      "  if not IsKernelExtensionAvailable(\"Browse\", \"ncurses\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/cddinterface/Pac\\\n",
      "kageInfo.g:85\n",
      "    if not IsKernelExtensionAvailable(\"CddInterface\") then\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/crypting/Package\\\n",
      "Info.g:82\n",
      "   if not IsKernelExtensionAvailable(\"crypting\") then\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/curlinterface/Pa\\\n",
      "ckageInfo.g:114\n",
      "  if not IsKernelExtensionAvailable(\"curlinterface\", \"curl\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/cvec/PackageInfo\\\n",
      ".g:114\n",
      "  if not IsKernelExtensionAvailable(\"cvec\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/datastructures/P\\\n",
      "ackageInfo.g:125\n",
      "  if not IsKernelExtensionAvailable(\"datastructures\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/deepthought/Pack\\\n",
      "ageInfo.g:89\n",
      "  if not IsKernelExtensionAvailable(\"DeepThought\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/edim/PackageInfo\\\n",
      ".g:60\n",
      "  if not IsKernelExtensionAvailable(\"EDIM\",\"ediv\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/ferret/PackageIn\\\n",
      "fo.g:76\n",
      "  if not IsKernelExtensionAvailable(\"ferret\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/float/PackageInf\\\n",
      "o.g:69\n",
      "  if not IsKernelExtensionAvailable(\"float\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/gauss/PackageInf\\\n",
      "o.g:103\n",
      "  if not IsKernelExtensionAvailable(\"gauss\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/io/PackageInfo.g\\\n",
      ":103\n",
      "  if not IsKernelExtensionAvailable(\"io\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/json/PackageInfo\\\n",
      ".g:79\n",
      "   if IsKernelExtensionAvailable(\"json\") = false then\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/normalizinterfac\\\n",
      "e/PackageInfo.g:89\n",
      "    if not IsKernelExtensionAvailable(\"NormalizInterface\") then\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/orb/PackageInfo.\\\n",
      "g:141\n",
      "    if IsKernelExtensionAvailable(\"orb\") = false then\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/profiling/Packag\\\n",
      "eInfo.g:79\n",
      "  if not IsKernelExtensionAvailable(\"profiling\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/semigroups/Packa\\\n",
      "geInfo.g:417\n",
      "  if not IsKernelExtensionAvailable(\"semigroups\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Syntax warning: Unbound global variable in /usr/share/gap/pkg/zeromqinterface/\\\n",
      "PackageInfo.g:102\n",
      "  if not IsKernelExtensionAvailable(\"zeromqinterface\") then\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torchopt\n",
    "import functorch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "\n",
    "\n",
    "import os, sys\n",
    "HOME = os.environ['HOME']  # change if necessary\n",
    "sys.path.append(f'{HOME}/Finite-groups/src')\n",
    "from model import MLP3, MLP4, InstancedModule\n",
    "from utils import *\n",
    "from group_data import *\n",
    "from model_utils import *\n",
    "from train import Parameters\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticModel(nn.Module):\n",
    "    def __init__(self, A):\n",
    "        super().__init__()\n",
    "        self.A = A\n",
    "        self.x = nn.Parameter(t.randn(A.shape[0]))\n",
    "\n",
    "    def forward(self):\n",
    "        return (self.x.T @ self.A @ self.x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16905/538059995.py:7: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  model_f, init_params = functorch.make_functional(model)\n"
     ]
    }
   ],
   "source": [
    "ADAM_CFG = {\n",
    "    'lr': 0.01,\n",
    "}\n",
    "B = t.randn(5, 5)\n",
    "model = QuadraticModel(B.T @ B + 0.01 * t.eye(5))\n",
    "opt = torchopt.FuncOptimizer(torchopt.adam(**ADAM_CFG))\n",
    "model_f, init_params = functorch.make_functional(model)\n",
    "\n",
    "def train(init_params, iters):\n",
    "    params = init_params\n",
    "    for _ in tqdm(range(iters)):\n",
    "        loss = model_f(params)\n",
    "        params = opt.step(loss, params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f39fb9dbdbf469cbba0451b52879bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.8728, -0.0568, -0.0088,  0.0194,  0.0596],\n",
       "          [-0.0096,  0.9777, -0.0029,  0.0046,  0.0027],\n",
       "          [-0.0029, -0.0059,  0.9853,  0.0128,  0.0014],\n",
       "          [ 0.0054,  0.0075,  0.0104,  0.9740, -0.0010],\n",
       "          [ 0.2598,  0.0741,  0.0190, -0.0174,  0.8710]],\n",
       "         grad_fn=<ViewBackward0>),),)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.func.jacrev(partial(train, iters=10))(init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection size: 576/576 (1.00)\n",
      "Added 576 elements from intersection\n",
      "Added 0 elements from group 0: S(4)\n",
      "Taking random subset: 230/576 (0.40)\n",
      "Train set size: 230/576 (0.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96809/1958598494.py:37: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.make_functional` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.functional_call` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n",
      "  model_f, init_params = functorch.make_functional(model)\n"
     ]
    }
   ],
   "source": [
    "PARAMS = Parameters(\n",
    "    instances=1,\n",
    "    embed_dim=32,\n",
    "    hidden_size=32,\n",
    "    group_string='S(4)',\n",
    "    model='MLP2',\n",
    "    unembed_bias=True,\n",
    "    weight_decay=2e-5,\n",
    "    train_frac=0.4,\n",
    ")\n",
    "\n",
    "t.manual_seed(PARAMS.seed)\n",
    "np.random.seed(PARAMS.seed)\n",
    "random.seed(PARAMS.seed)\n",
    "group_dataset = GroupData(params=PARAMS)\n",
    "model = MODEL_DICT[PARAMS.model](params=PARAMS).to(device)\n",
    "\n",
    "batch_size = len(group_dataset)\n",
    "# sampler = RandomSampler(group_dataset, replacement=PARAMS.replacement)\n",
    "train_loader = DataLoader(\n",
    "    dataset=group_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True,\n",
    "    drop_last=True,\n",
    "    # sampler=sampler,\n",
    ")\n",
    "\n",
    "# TODO: bias params should not get weight decay (to match with train.py)\n",
    "# But probably doesn't matter much\n",
    "opt = torchopt.FuncOptimizer(\n",
    "    torchopt.adam(\n",
    "        weight_decay=PARAMS.weight_decay,\n",
    "        lr=PARAMS.lr,\n",
    "        betas=[PARAMS.beta1, PARAMS.beta2],\n",
    "    )\n",
    ")\n",
    "model_f, init_params = functorch.make_functional(model)\n",
    "param_shapes = [p.shape for p in init_params]\n",
    "\n",
    "def flatten(params):\n",
    "    return t.cat([p.flatten() for p in params])\n",
    "\n",
    "def unflatten(flat_params, shapes):\n",
    "    params = []\n",
    "    i = 0\n",
    "    for shape in shapes:\n",
    "        size = np.prod(shape)\n",
    "        params.append(flat_params[i:i+size].reshape(shape))\n",
    "        i += size\n",
    "    return tuple(params)\n",
    "\n",
    "def train(flat_init_params, epochs):\n",
    "    params = unflatten(flat_init_params, param_shapes)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for x, z in train_loader:\n",
    "            x = x.to(device)\n",
    "            z = z.to(device)\n",
    "            output = model_f(params, x)\n",
    "            loss = get_cross_entropy(output, z)\n",
    "            params = opt.step(loss, params)\n",
    "    return flatten(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 42.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4376, 4376])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_jac = t.func.jacrev(partial(train, epochs=2))(flatten(init_params))\n",
    "train_jac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.allclose(train_jac, t.eye(train_jac.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9999e-01,  1.3763e-06, -4.0296e-06, -1.3249e-06,  1.9222e-06,\n",
       "         -2.1517e-06, -3.1543e-07, -4.1833e-06,  1.4107e-06, -5.3752e-07],\n",
       "        [-4.8221e-04,  9.9739e-01,  3.9119e-04,  3.7268e-05,  7.1354e-04,\n",
       "          3.7365e-04, -1.7262e-04,  8.6500e-05, -8.3316e-06, -4.8661e-04],\n",
       "        [-1.8565e-05,  2.0593e-06,  9.9971e-01, -1.7941e-05, -6.4639e-05,\n",
       "         -1.9601e-05,  4.4963e-05, -7.7958e-06,  1.1871e-05, -4.6507e-05],\n",
       "        [-7.2219e-06, -5.2852e-06, -4.3495e-06,  9.9997e-01,  1.3083e-05,\n",
       "          7.3948e-06,  6.3212e-06,  8.7692e-07,  7.7940e-06, -4.9355e-06],\n",
       "        [ 1.8482e-05,  2.4371e-05, -6.2865e-05,  3.7429e-05,  9.9972e-01,\n",
       "         -1.0835e-05, -3.7380e-05,  3.7753e-05, -1.2660e-05,  5.4999e-05],\n",
       "        [ 1.7986e-05,  1.9715e-05, -1.1433e-05,  1.5340e-05, -2.9916e-05,\n",
       "          9.9988e-01,  2.0188e-05,  2.9279e-06, -7.8407e-06, -6.9179e-06],\n",
       "        [ 2.2611e-06,  1.6279e-06,  5.5052e-06,  2.8162e-08, -6.1209e-07,\n",
       "          4.5893e-08,  9.9999e-01,  1.8620e-06,  9.9166e-07, -3.7672e-07],\n",
       "        [ 5.1174e-06, -2.7121e-06, -2.1911e-06, -7.7828e-07,  7.2341e-06,\n",
       "          1.5259e-06,  1.3794e-05,  9.9997e-01,  2.3391e-06,  2.8177e-07],\n",
       "        [ 1.9866e-04, -5.9454e-06,  2.9027e-05, -2.1883e-05, -6.0055e-05,\n",
       "         -3.2455e-05,  1.3314e-04, -1.4646e-04,  9.9904e-01,  1.2716e-04],\n",
       "        [-7.3920e-04, -7.0550e-04, -5.0731e-04, -7.2670e-05,  1.0399e-03,\n",
       "         -7.1126e-05, -5.0134e-04,  1.5741e-03,  7.0657e-04,  9.9473e-01]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_jac[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 24, 32]),\n",
       " torch.Size([1, 24, 32]),\n",
       " torch.Size([1, 32, 32]),\n",
       " torch.Size([1, 32, 32]),\n",
       " torch.Size([1, 32, 24]),\n",
       " torch.Size([1, 24])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.shape for t in init_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 125.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0008], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = train(init_params, epochs=1000)\n",
    "x, z = next(iter(train_loader))\n",
    "x = x.to(device)\n",
    "z = z.to(device)\n",
    "output = model_f(params, x)\n",
    "get_cross_entropy(output, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0008], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_addition-BDyFuYvs-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
