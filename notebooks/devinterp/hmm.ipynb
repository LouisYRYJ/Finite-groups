{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# HOME = os.environ['HOME']  # change if necessary\n",
    "HOME = '/workspace/'\n",
    "sys.path.append(f'{HOME}/wilson/Finite-groups/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from itertools import product\n",
    "from jaxtyping import Float\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.graph_objects as go\n",
    "import copy\n",
    "import math\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "from einops import repeat\n",
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from hmmlearn import hmm\n",
    "\n",
    "\n",
    "from model import MLP3, MLP4, InstancedModule\n",
    "from utils import *\n",
    "from group_data import *\n",
    "from model_utils import *\n",
    "from group_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.set_grad_enabled(False)\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace//wilson/Finite-groups/src/utils.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  losses = [t.load(f) for f in loss_files]\n",
      "/workspace//wilson/Finite-groups/src/utils.py:160: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  losses = [t.load(f) for f in loss_files]\n"
     ]
    }
   ],
   "source": [
    "# MODEL_DIR = f'{HOME}/models/2025-01-08_03-15-59_S4_A4x2_MLP2_32_ubias_wd2e-4_BIG_hmmmetrics'\n",
    "# MODEL_DIR = f'{HOME}/models/2025-01-08_03-15-59_S4_A4x2_MLP2_32_ubias_wd2e-4_BIG_hmmmetrics'\n",
    "MODEL_DIR = f'{HOME}/models/2025-01-08_22-01-30_S5_A5x2_MLP2_128_ubias_wd2e-5_hmmmetrics'\n",
    "losses = load_loss_trajectory(MODEL_DIR)\n",
    "hmm_metrics = load_hmm_trajectory(MODEL_DIR)\n",
    "hmm_keys = list(hmm_metrics.keys())\n",
    "hmm_metrics = t.stack([hmm_metrics[k] for k in hmm_keys], dim=1)  # instance hmm_metric epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit HMM\n",
    "See Hu et al. \"Latent state models of training dynamics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2001])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses['G0_acc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_THRESH = 0.99\n",
    "# DATA_SIZE = 30\n",
    "DATA_SIZE = 100\n",
    "g0_grokked = losses['G0_acc'][:,-1] > ACC_THRESH\n",
    "g1_grokked = losses['G1_acc'][:,-1] > ACC_THRESH\n",
    "none_grokked = ~(g0_grokked | g1_grokked)\n",
    "g0_hmm_metrics = hmm_metrics[g0_grokked]\n",
    "g1_hmm_metrics = hmm_metrics[g1_grokked]\n",
    "none_hmm_metrics = hmm_metrics[none_grokked]\n",
    "balanced_hmm_metrics = t.concat([g0_hmm_metrics[:DATA_SIZE], g1_hmm_metrics[:DATA_SIZE], none_hmm_metrics[:DATA_SIZE]], axis=0)\n",
    "balanced_hmm_metrics = balanced_hmm_metrics[t.randperm(balanced_hmm_metrics.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * balanced_hmm_metrics.shape[0])\n",
    "train_data = balanced_hmm_metrics[:train_size].cpu().numpy()\n",
    "train_lengths = [train_data.shape[-1]] * train_data.shape[0]\n",
    "train_data = einops.rearrange(train_data, 'instance metric epoch -> (instance epoch) metric')\n",
    "train_data = (train_data - train_data.mean(axis=0)) / train_data.std(axis=0)\n",
    "\n",
    "test_data = balanced_hmm_metrics[train_size:].cpu().numpy()\n",
    "test_lengths = [test_data.shape[-1]] * test_data.shape[0]\n",
    "test_data = einops.rearrange(test_data, 'instance metric epoch -> (instance epoch) metric')\n",
    "test_data = (test_data - train_data.mean(axis=0)) / train_data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:00<00:50, 19.86it/s]\n",
      "  0%|          | 2/1000 [00:00<00:49, 20.12it/s]\n",
      "  0%|          | 2/1000 [00:00<00:49, 20.32it/s]\n",
      "  0%|          | 2/1000 [00:00<00:48, 20.49it/s]\n",
      "  0%|          | 2/1000 [00:00<00:49, 20.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -215,053,228,291\n",
      "n_components 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [00:01<01:26, 11.32it/s]\n",
      "  2%|▏         | 22/1000 [00:01<01:27, 11.16it/s]\n",
      "  2%|▏         | 22/1000 [00:01<01:26, 11.26it/s]\n",
      "  2%|▏         | 22/1000 [00:01<01:26, 11.33it/s]\n",
      "  2%|▏         | 22/1000 [00:01<01:28, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -226,407,291,751\n",
      "n_components 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [00:03<02:18,  7.06it/s]\n",
      "  2%|▏         | 21/1000 [00:03<02:25,  6.74it/s]\n",
      "  2%|▏         | 20/1000 [00:02<02:20,  7.00it/s]\n",
      "  2%|▏         | 21/1000 [00:03<02:21,  6.93it/s]\n",
      "  2%|▎         | 25/1000 [00:03<02:18,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -438,905,741,430\n",
      "n_components 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 46/1000 [00:08<03:02,  5.22it/s]\n",
      "  5%|▌         | 53/1000 [00:10<03:11,  4.95it/s]\n",
      "  4%|▍         | 45/1000 [00:09<03:13,  4.94it/s]\n",
      "  4%|▍         | 43/1000 [00:08<03:10,  5.01it/s]\n",
      "  5%|▍         | 46/1000 [00:09<03:09,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -445,301,741,327\n",
      "n_components 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 63/1000 [00:16<04:12,  3.71it/s]\n",
      "  3%|▎         | 30/1000 [00:08<04:33,  3.54it/s]\n",
      "  4%|▎         | 36/1000 [00:09<04:24,  3.65it/s]\n",
      "  3%|▎         | 34/1000 [00:09<04:30,  3.56it/s]\n",
      "  3%|▎         | 31/1000 [00:08<04:36,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -460,160,282,262\n",
      "n_components 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [00:15<05:37,  2.84it/s]\n",
      "  4%|▎         | 36/1000 [00:12<05:30,  2.91it/s]\n",
      " 12%|█▏        | 116/1000 [00:35<04:32,  3.25it/s]\n",
      "  5%|▍         | 48/1000 [00:16<05:35,  2.83it/s]\n",
      " 12%|█▏        | 115/1000 [00:37<04:50,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -460,529,648,192\n",
      "n_components 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 94/1000 [00:37<06:00,  2.51it/s]\n",
      "  8%|▊         | 83/1000 [00:33<06:09,  2.48it/s]\n",
      "  9%|▉         | 91/1000 [00:37<06:13,  2.43it/s]\n",
      "  7%|▋         | 74/1000 [00:29<06:03,  2.55it/s]\n",
      " 10%|▉         | 96/1000 [00:36<05:45,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -638,794,650,372\n",
      "n_components 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 124/1000 [00:56<06:41,  2.18it/s]\n",
      " 13%|█▎        | 126/1000 [00:59<06:51,  2.13it/s]\n",
      " 10%|█         | 100/1000 [00:48<07:13,  2.08it/s]\n",
      " 12%|█▏        | 119/1000 [00:54<06:43,  2.18it/s]\n",
      " 22%|██▏       | 218/1000 [01:33<05:33,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: -764,796,565,289\n",
      "n_components 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 137/1000 [01:12<07:35,  1.90it/s]\n",
      "  7%|▋         | 66/1000 [00:35<08:27,  1.84it/s]\n",
      "  9%|▉         | 90/1000 [00:49<08:16,  1.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m hmm\u001b[38;5;241m.\u001b[39mGaussianHMM(n_components\u001b[38;5;241m=\u001b[39mn_components, covariance_type\u001b[38;5;241m=\u001b[39mcov_type, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(test_data, lengths\u001b[38;5;241m=\u001b[39mtest_lengths)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# bic = model.bic(test_data, lengths=test_lengths)   # bic only makes sense to evaluate on train data...\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/group_addition-jQUm9okg-py3.11/lib/python3.11/site-packages/hmmlearn/base.py:486\u001b[0m, in \u001b[0;36m_AbstractHMM.fit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter)):\n\u001b[0;32m--> 486\u001b[0m     stats, curr_logprob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_estep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Compute lower bound before updating model parameters\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(curr_logprob)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/group_addition-jQUm9okg-py3.11/lib/python3.11/site-packages/hmmlearn/base.py:769\u001b[0m, in \u001b[0;36m_AbstractHMM._do_estep\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    765\u001b[0m     lattice, logprob, posteriors, fwdlattice, bwdlattice \u001b[38;5;241m=\u001b[39m impl(sub_X)\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;66;03m# Derived HMM classes will implement the following method to\u001b[39;00m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;66;03m# update their probability distributions, so keep\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;66;03m# a single call to this method for simplicity.\u001b[39;00m\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accumulate_sufficient_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m     curr_logprob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m logprob\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats, curr_logprob\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/group_addition-jQUm9okg-py3.11/lib/python3.11/site-packages/hmmlearn/_emissions.py:145\u001b[0m, in \u001b[0;36mBaseGaussianHMM._accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_accumulate_sufficient_statistics\u001b[39m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m, stats, X, lattice, posteriors, fwdlattice, bwdlattice):\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accumulate_sufficient_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_sufficient_statistics_for_mean():\n\u001b[1;32m    152\u001b[0m         stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m posteriors\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/group_addition-jQUm9okg-py3.11/lib/python3.11/site-packages/hmmlearn/base.py:703\u001b[0m, in \u001b[0;36m_AbstractHMM._accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mUpdate sufficient statistics from a given sample.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m    forward and backward probabilities.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m impl \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_sufficient_statistics_scaling,\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulate_sufficient_statistics_log,\n\u001b[1;32m    701\u001b[0m }[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimplementation]\n\u001b[0;32m--> 703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposteriors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposteriors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbwdlattice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/group_addition-jQUm9okg-py3.11/lib/python3.11/site-packages/hmmlearn/base.py:740\u001b[0m, in \u001b[0;36m_AbstractHMM._accumulate_sufficient_statistics_log\u001b[0;34m(self, stats, X, lattice, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m log_xi_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_hmmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_xi_sum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransmat_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbwdlattice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlattice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    743\u001b[0m     stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrans\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(log_xi_sum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "bics = []\n",
    "# cov_type = 'full'\n",
    "cov_type = 'diag'\n",
    "for n_components in range(1, 30):\n",
    "    print('n_components', n_components)\n",
    "    # best_bic = float('inf')\n",
    "    best_score = float('-inf')\n",
    "    for seed in range(5):\n",
    "        model = hmm.GaussianHMM(n_components=n_components, covariance_type=cov_type, n_iter=1000, random_state=seed)\n",
    "        model.fit(train_data, lengths=train_lengths)\n",
    "        score = model.score(test_data, lengths=test_lengths)\n",
    "        # bic = model.bic(test_data, lengths=test_lengths)   # bic only makes sense to evaluate on train data...\n",
    "        best_score = max(best_score, score)\n",
    "        # best_bic = min(best_bic, bic)\n",
    "    print(f'score: {int(best_score):,}')\n",
    "    # print('bic', best_bic)\n",
    "    scores.append(best_score)\n",
    "    # bics.append(best_bic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6558513275.564452"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(test_data, lengths=test_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(13117029247.089188)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bic(test_data, lengths=test_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_addition-jQUm9okg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
