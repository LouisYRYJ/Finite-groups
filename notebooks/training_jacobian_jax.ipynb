{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "# HOME = os.environ['HOME']  # change if necessary\n",
    "HOME = '/workspace/wilson'\n",
    "sys.path.append(f'{HOME}/Finite-groups/src')\n",
    "# from model import MLP3, MLP4, InstancedModule\n",
    "from utils import *\n",
    "from group_data import *\n",
    "# from model_utils import *\n",
    "from train import Parameters\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from flax import linen as nn, struct\n",
    "from flax.training.train_state import TrainState\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from optax import (\n",
    "    softmax_cross_entropy_with_integer_labels as xent\n",
    ")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jaxtyping import Int, Array, Float\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "from frozendict import frozendict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP2(nn.Module):\n",
    "    # right now this hardcodes a lot of things to the defaults\n",
    "    cfg: frozendict\n",
    "    N: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, a: Int[Array, 'batch entry']) -> Float[Array, 'batch vocab']:\n",
    "        a1, a2 = a[...,0], a[...,1]\n",
    "        embedding_init = nn.initializers.normal(stddev=1.0)\n",
    "        # In the torch code, uniform bounds are \\pm 1/sqrt(fan_in) * 1/sqrt(2) (kaiming_uniform with scale=1/sqrt(2) to account for 2 input arguments)\n",
    "        # This corresponds to std = 1/sqrt(6) * 1/sqrt(fan_in)\n",
    "        # variance_scaling sets std = sqrt(scale) / sqrt(fan_in)\n",
    "        # So we should set scale = 1/6 to get the same initialization\n",
    "        kernel_init = nn.initializers.variance_scaling(mode='fan_in', distribution='uniform', scale=1/6)\n",
    "        # This differs from the pytorch version\n",
    "        # In pytorch we had uniform distribution over [-1/sqrt(fan_in), 1/sqrt(fan_in)]\n",
    "        # Here we use constant initialization to 1/N\n",
    "        # The former matches the default for torch.nn.Linear, but it doesn't seem motivated by anything\n",
    "        # and anyways I don't know how to recreate it with flax...\n",
    "        bias_init = nn.initializers.constant(1 / self.N)\n",
    "        # bias_init = nn.initializers.uniform(scale=1/np.sqrt(self.params.hidden_size))\n",
    "        x1 = nn.Embed(self.N, self.cfg['embed_dim'], embedding_init=embedding_init, name='embedding_left')(a1)\n",
    "        x2 = nn.Embed(self.N, self.cfg['embed_dim'], embedding_init=embedding_init, name='embedding_right')(a2)\n",
    "        h1 = nn.Dense(self.cfg['hidden_size'], use_bias=False, kernel_init=kernel_init, name='linear_left')(x1)\n",
    "        h2 = nn.Dense(self.cfg['hidden_size'], use_bias=False, kernel_init=kernel_init, name='linear_right')(x2)\n",
    "        h = h1 + h2\n",
    "        act = nn.relu(h)\n",
    "        return nn.Dense(self.N, kernel_init=kernel_init, bias_init=bias_init, name='unembedding')(act)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_apply_full(model, unraveler):\n",
    "    \"\"\"Apply function that takes raveled params vector.\"\"\"\n",
    "    def apply_full(raveled, x):\n",
    "        params = unraveler(raveled)\n",
    "        return model.apply(params, x)\n",
    "    \n",
    "    return apply_full\n",
    "\n",
    "def loss_fn(params, apply_fn, x, z):\n",
    "    logits = apply_fn(params, x)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    loss = xent(logits, z).mean()\n",
    "    acc = (preds == z).mean()\n",
    "    return loss, acc\n",
    "\n",
    "def wd_loss_fn(params, apply_fn, x, z, wd):\n",
    "    loss, acc = loss_fn(params, apply_fn, x, z)\n",
    "    return loss + wd / 2 * jnp.linalg.norm(params) ** 2, (loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(3, 4))\n",
    "def train(ravel_params, x, z, apply_fn, cfg: frozendict):\n",
    "    tx = optax.adam(\n",
    "        learning_rate=cfg['lr'],\n",
    "        b1=cfg['beta1'],\n",
    "        b2=cfg['beta2'],\n",
    "    )\n",
    "    \n",
    "    state = TrainState.create(apply_fn=apply_fn, params=ravel_params, tx=tx)\n",
    "    loss_and_grad = jax.value_and_grad(wd_loss_fn, has_aux=True)\n",
    "    \n",
    "    # TODO: support batching (right now only full batch)\n",
    "    def epoch_step(state:TrainState, epoch) -> tuple[TrainState, tuple[jnp.ndarray, jnp.ndarray]]:\n",
    "        (_, (loss, acc)), grads = loss_and_grad(state.params, state.apply_fn, x, z, cfg['weight_decay'])\n",
    "        return state.apply_gradients(grads=grads), (loss, acc)\n",
    "\n",
    "    state, (train_loss, train_acc) = jax.lax.scan(epoch_step, state, jnp.arange(cfg['epochs']))\n",
    "    return state.params, (train_loss, train_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection size: 576/576 (1.00)\n",
      "Added 576 elements from intersection\n",
      "Added 0 elements from group 0: S(4)\n",
      "Taking random subset: 345/576 (0.60)\n",
      "Train set size: 345/576 (0.60)\n"
     ]
    }
   ],
   "source": [
    "device = jax.devices('gpu')[0]\n",
    "\n",
    "PARAMS = Parameters(\n",
    "    instances=1,\n",
    "    # embed_dim=32,\n",
    "    # hidden_size=32,\n",
    "    # group_string='S(4)',\n",
    "    embed_dim=32,\n",
    "    hidden_size=32,\n",
    "    group_string='S(4)',\n",
    "    model='MLP2',\n",
    "    unembed_bias=True,\n",
    "    weight_decay=2e-4,\n",
    "    train_frac=0.6,\n",
    "    epochs=4000,\n",
    ")\n",
    "\n",
    "t.manual_seed(PARAMS.seed)\n",
    "np.random.seed(PARAMS.seed)\n",
    "random.seed(PARAMS.seed)\n",
    "\n",
    "group_dataset = GroupData(params=PARAMS)\n",
    "train_data = jnp.array(group_dataset.train_data, device=device)\n",
    "\n",
    "# Needs to be hashable for jax jit\n",
    "CFG = frozendict(\n",
    "    {k: v for k, v in PARAMS.__dict__.items() if k not in ['delta_frac']}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, z_train = train_data[:,:2], train_data[:,2]\n",
    "model = MLP2(CFG, group_dataset.N)\n",
    "init_params = model.init(jax.random.key(PARAMS.seed), x_train)\n",
    "init_ravel_params, unraveler = ravel_pytree(init_params)\n",
    "apply_fn = make_apply_full(model, unraveler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.00673234, dtype=float32), Array(1., dtype=float32))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ravel_params, (train_loss, train_acc) = train(init_ravel_params, x_train, z_train, apply_fn, CFG)\n",
    "train_loss[-1], train_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.01418004, dtype=float32), Array(1., dtype=float32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = jnp.asarray(list(product(range(group_dataset.N), repeat=2)), device=device)\n",
    "z_test = einops.rearrange(jnp.asarray(group_dataset.groups[0].cayley_table, device=device), 'i j -> (i j)')\n",
    "loss_fn(final_ravel_params, apply_fn, x_test, z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac, _ = jax.jit(jax.jacfwd(train, has_aux=True), static_argnums=(3, 4,))(init_ravel_params, x_train, z_train, apply_fn, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.save('/workspace/wilson/Finite-groups/data/trainjac_S4_d32_wd2e-4_epochs4000.npy', jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V = jnp.linalg.svd(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(S)\n",
    "plt.axhline(1, linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group_addition-jQUm9okg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
